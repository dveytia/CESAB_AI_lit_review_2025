---
title: "01_screening_examples"
author: "Devi Veytia"
date: "2025-08-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





# Example: Fine-tuning an encoder model for text classification

```{r load the virtual environment}
# Load reticulate and set up Python environment
# install.packages("reticulate")
library(reticulate)


# Check if you already have the virual environment
env_exists <- virtualenv_exists(envname = "lit_review")


# Load it if you do, install if you don't
if(env_exists){
  use_virtualenv("lit_review", required = TRUE)
}else{
  # Create / use a virtualenv for Python
  virtualenv_create("lit_review", python_version = "3.8")
  use_virtualenv("lit_review", required = TRUE)
  
  # Install dependencies (run once)
  py_install(c("transformers", "torch", "datasets", "scikit-learn","evaluate","accelerate>=0.26.0",
               "langchain","langchain-core","typing","langchain-huggingface","langchain-ollama"))
  
}


```

```{python load modules}

from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments, pipeline
from datasets import Dataset
import evaluate
import numpy as np
import sklearn
import pandas as pd

```


## Divide data for cross validation

Create example data with the following columns:

* id - a unique row identifier
* text - example text (a sentence) which is about biodiversity or some other topic
* environment_true - a human label of whether the text is about the environment or not (example screening decision)


```{r make example training data in r}
# Example dataframe with id, text, and environment_true
set.seed(123)
df <- data.frame(
  id = 1:20,
  text = c(
    "Biodiversity is crucial for healthy ecosystems",
    "The weather today is sunny and warm",
    "Deforestation reduces species diversity",
    "I love playing soccer with friends",
    "Conservation protects endangered species",
    "The stock market rose by 2 percent",
    "Pollinators are essential for crop production",
    "I enjoy listening to jazz music",
    "Marine biodiversity supports fisheries",
    "The new smartphone was released today",
    "Wetlands store carbon and prevent flooding",
    "I baked a cake yesterday",
    "Habitat loss threatens many animals",
    "She went shopping at the mall",
    "Genetic diversity strengthens ecosystems",
    "He bought a new car last week",
    "Ecosystem services benefit human health",
    "The movie was exciting and fun",
    "Forests provide habitat and clean air",
    "Iâ€™m learning to play guitar"
  ),
  environment_true = c(1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0)
)

df$environment_true <- as.integer(df$environment_true)
head(df)

```


For simplicity we will just split the data into one training and test set, but be aware that more often k-fold cross validation is used (the data are split into K number of splits, and then each split is used alternatively as the testing set while the others are the training) 
```{python Convert R dataframe to HuggingFace Dataset and split into training and testing}

train_df = r.df.sample(frac=0.7, random_state=42)   # 70% train
test_df  = r.df.drop(train_df.index)
train_ds = Dataset.from_pandas(train_df)
test_ds  = Dataset.from_pandas(test_df)

```

## Define evaluation metrics

```{python Define evaluation metrics}

metric_accuracy = evaluate.load("accuracy")
metric_precision = evaluate.load("precision")
metric_recall = evaluate.load("recall")
metric_f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": metric_accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "precision": metric_precision.compute(predictions=preds, references=labels, average="binary")["precision"],
        "recall": metric_recall.compute(predictions=preds, references=labels, average="binary")["recall"],
        "f1": metric_f1.compute(predictions=preds, references=labels, average="binary")["f1"],
    }


```


## Tokenize the data


```{python define classification model and tokenizer}

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2,
    id2label={0:"not environment", 1:"environment"},
    label2id={"not environment":0, "environment":1}
)

# Load tokenizer + model with custom labels
# Note a warning will pop up when the model is loaded -- this is normal
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")


```



```{python tokenize - convert text to integers}
# Tokenize
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_ds = train_ds.map(tokenize, batched=True)
test_ds  = test_ds.map(tokenize, batched=True)

# Rename labels and ensure they are integers
train_ds = train_ds.rename_column("environment_true","labels")
test_ds  = test_ds.rename_column("environment_true","labels")
train_ds.set_format("torch", columns=["input_ids","attention_mask","labels"])
test_ds.set_format("torch", columns=["input_ids","attention_mask","labels"])


```


## Calculate attention weights & train the classification layer 

Note that batch size, number of epochs, weight decay, etc are just a few of the hyperparameters that you can optimize (others include loss functions). If you plan on using this approach yourself, you should familiarize yourself with how different combinations of hyperparameters can be looped through to select for the best model configuration using nested cross validation.

But here we will just select arbitrary hyperparameter values as an example. 

```{python fine-tune your model on the training data}

training_args = TrainingArguments(
    output_dir="./outputs/checkpoints", # Output directory for model checkpoints
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=2,
    weight_decay=0.01,
    learning_rate=5e-5,  
    logging_dir="./outputs/logs",
    eval_strategy = "epoch",
    save_strategy="epoch",  # Save checkpoints at the end of each epoch
    save_total_limit = 1,
    load_best_model_at_end=True,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    processing_class=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
metrics = trainer.evaluate()
print("Evaluation metrics:", metrics)

# # Save fine-tuned model
# model.save_pretrained("./data/derived-data/finetuned_distilbert_biodiv")
# tokenizer.save_pretrained("./data/derived-data/finetuned_distilbert_biodiv")
```


Then, as an example of how you would get predictions for unseen text, I'll load the model and print the predictions for one example senetence:
```{python load the saved model and use it to predict the classification for some input text}
# Pipeline prediction on a test sentence
nlp = pipeline("text-classification", model="./data/derived-data/finetuned_distilbert_biodiv", tokenizer="./data/derived-data/finetuned_distilbert_biodiv")
example_text = "Conservation protects endangered species"

print("Example prediction:", nlp(example_text))

```

```{python clean environment}
import gc

del nlp
del tokenizer
del model
gc.collect()

```

Here we use the model for screening, but you can easily adapt this code for multiple labels (e.g. for coding) by changing the num_labels parameter when loading the model.

Here's [another great tutorial for text classification from Hugging Face](https://huggingface.co/docs/transformers/en/tasks/sequence_classification)


FYI:
In real life, we don't usually have an even number of examples of inclusions and exclusions. Often the number of inclusions << exclusions. In this scenario, you want to make sure that you have an adequate number of inclusions in your training data (which can be augmented using keyword searching, active learning, etc). If you use this approach, make sure that only randomly sampled documents are used in the test set.



