---
title: "02_coding_examples"
author: "Devi Veytia"
date: "2025-08-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Example 1: Coding using a generative LLM

In this script, it performs noticeably better if you can use the ollama models. To do so, you will need to download ollama: https://ollama.com/

Then, open your computer's terminal, and run:
> ollama pull deepseek-r1:7b
> ollama pull phi3:3.8b
> ollama pull deepseek-r1:1.5b

This will download the model weights locally to your computer. You will then be able to access them from python using langchain. 



```{r load the packages}
# Load reticulate and set up Python environment
# install.packages("reticulate")
library(reticulate)


# Check if you already have the virual environment
env_exists <- virtualenv_exists(envname = "lit_review")


# Load it if you do, install if you don't
if(env_exists){
  use_virtualenv("lit_review", required = TRUE)
}else{
  # Create / use a virtualenv for Python
  virtualenv_create("lit_review", python_version = "3.8")
  use_virtualenv("lit_review", required = TRUE)
  
  # Install dependencies (run once)
  py_install(c("transformers", "torch", "datasets", "scikit-learn","evaluate","accelerate>=0.26.0",
               "langchain","langchain-core","typing","langchain-huggingface","langchain-ollama","faiss-cpu"))
  
 
}


```


```{python load modules}
from transformers import pipeline
from typing import Union, List, Literal, Annotated
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.output_parsers import OutputFixingParser

```

```{python define the google flan as the generative llm if you don't have ollama}
from langchain_huggingface import HuggingFacePipeline


# Note warnings pop up when you load the model -- this is normal
pipe = pipeline("text2text-generation", model="google/flan-t5-base", device=-1)  
model1 = HuggingFacePipeline(pipeline=pipe)
model2 = HuggingFacePipeline(pipeline=pipe)

```

```{python even better -- use an ollama model}
from langchain_ollama.llms import OllamaLLM
model1 = OllamaLLM(model="deepseek-r1:7b", temperature=0.2)
model2 = OllamaLLM(model="phi3:3.8b", temperature=0.2)
```



```{python start with a simple prompt for the llm}
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            You are a scientist performing a scientific literature review.
            
            ## Task: 
            Given a text extract from a scientific article, identify the environmental impacts being studied in the article. 
            
            
            
            ## Instructions:
            Return a short summary of any environmental impacts identified in 10 words or less. DO NOT MAKE ANYTHING UP. 
            
            """
        ),
        ("human", "Article text: {context}")
    ]
)

input_text = "This study researches the impacts of marine noise generated from pile driving on the behaviour and habitat use of marine mammals."


chain = prompt | model1 

result = chain.invoke({
    "context": input_text
})

print(result)

```



```{python Create your own -- play with different prompts and input texts}
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            
            Enter your own prompt here
            
            """
        ),
        ("human", "Article text: {context}")
    ]
)

input_text = "Enter your own text here"


chain = prompt | model1 

result = chain.invoke({
    "context": input_text
})

print(result)

```



```{python parsing structured data}

# Define a class for structuring your data
class Location(BaseModel):
    location: str = Field(
      default = "None",
      description="The location where the study was conducted"
      )
    year: str = Field(
      default = "None",
      description="The year when the study was conducted in the format yyyy"
      )
      

LocationParser = PydanticOutputParser(pydantic_object=Location)

prompt_Location = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            You are a scientific research assistant.
            
            ## Task: 
            Given text extract from a scientific article, identify where the study was conducted, and in what year. 
            
            ##Instructions:
            - DO NOT MAKE ANYTHING UP.
            - Wrap the output in `json` tags: {format_instructions}
            
            ## Examples:
            
            example_text: "A study was conducted in Montpellier, France in 2025"
            example_assistant: {{'location': "Hawaii", 'year': "2025"}}
            
            """
        ),
        ("human", "Article text: {context}")
    ]
).partial(format_instructions=LocationParser.get_format_instructions())


input_text = "The study was conducted in Hawaii in 2015."

chain = prompt_Location | model1 

result = chain.invoke({
    "context": input_text
})
result

fixingParser = OutputFixingParser.from_llm(
                parser=LocationParser,
                llm=model2
            )
            
            
result_structured = fixingParser.invoke(result)
print(result_structured.model_dump())

```





```{python parsing multi-label classification}

# Two or more distinct models
class ShipPressure(BaseModel):
    pressure_type: Literal['Ships'] = Field(..., description="Impacts from ships.")

class MREPressure(BaseModel):
    pressure_type: Literal['Marine_renewable_energy'] = Field(..., description="Impacts from marine renewables.")

# Combine into a Union
PressureUnion = Annotated[
    Union[ShipPressure, MREPressure],
    Field(discriminator = "pressure_type")
]

# Final model with discriminator
class PressureType(BaseModel):
    pressure_type: List[PressureUnion] = Field(
        default_factory=list,
        description="A list of pressure entries categorized by type."
    )

# Test the parser
PressureType.model_validate( {'pressure_type': [{'pressure_type': 'Ships'},{'pressure_type': 'Marine_renewable_energy'}]})


input_text = "This study was about environmental impacts from shipping and marine renewable energy."



PressureTypeParser = PydanticOutputParser(pydantic_object=PressureType)
PressureType_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            You are a scientist performing a scientific literature review.
            
            ##Task: identify and classify all relevant pressure types from the article text
            
            ##Instructions:
            Wrap the output in `json` tags: {format_instructions}
            
            ## Examples
            example_text: "This study was about environmental impacts from shipping and marine renewable energy."
            example_assistant: {{'pressure_type': [{{'pressure_type': 'Ships'}},{{'pressure_type': 'Marine_renewable_energy'}}]}}
            
            """
        ),
        ("human", "Article text: {context}")
    ]
).partial(format_instructions=PressureTypeParser.get_format_instructions())

PressureTypeFixingParser = OutputFixingParser.from_llm(
                parser=PressureTypeParser,
                llm=model2
            )
            
            

chain = PressureType_prompt | model1 

result_pressure = chain.invoke({
    "context": input_text
})
result_pressure

result_pressure_structured = fixingParser.invoke(result_pressure)

print(result_pressure_structured.model_dump())
```



# Example 2: RAG AI

Retrieval-Augmented Generation (RAG) allows an LLM to reference a specific knowledge base (context) outside of its training data sources before generating a response. This allows you to ask an LLM to answer questions about a source text.



```{python load rag ai modules and define embedding model}
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
from langchain.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
import os

# define the embeddings model
embeddings = OllamaEmbeddings(model="deepseek-r1:1.5b")
```

How to pick the best embedding model? You can see which ones are performing well on the [Hugging Face Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)


```{python question to ask from the article text}
qry = "Summarise in 10-100 words what this study is about"

```

```{r load in pdf text}
text <- pdftools::pdf_text(
  here::here("data/raw-data/test_pdf.pdf")
)
length(text) # The length of each vector corresponds to the number of pages in the PDF file.

# for brevity, let's just take the first page
text = text[1]
```


```{python split the text into chunks}
# Define a function to split the text into chunks
def split_text(text=list, chunk_size=500, chunk_overlap=50):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " "]
    )
    chunks = text_splitter.create_documents(text)
    return chunks

chunked_text = split_text(text=[r.text], chunk_size=500, chunk_overlap=50)


chunked_text_list = [doc.page_content for doc in chunked_text]

len(chunked_text)
len(chunked_text_list)

```

```{python get embeddings -- this may take a while so load in the next chunk, eval=FALSE}

def get_vectorstore(text_chunks, embeddings):
    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)
    return vectorstore


vectorstore = get_vectorstore(chunked_text_list, embeddings)

# vectorstore.save_local("./data/derived-data/test_pdf_faiss_vecs")
```

```{python load the vectorstore}

def load_vectorstore(faiss_path, embeddings):
    """Load FAISS vectorstore from disk if available."""
    if os.path.exists(faiss_path):
        return FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)
    return None


vectorstore = load_vectorstore("./data/derived-data/test_pdf_faiss_vecs", embeddings)

```

```{python retrive text chunks relevant to your question}
def retrieve_docs(query, vector_store, faissK=4):
    return vector_store.similarity_search(query, k=faissK)

def format_docs(related_documents):
    return "\n\n".join(doc.page_content for doc in related_documents)



related_chunks = retrieve_docs(query=qry, vector_store=vectorstore, faissK=5)
related_chunks = format_docs(related_chunks)
print(related_chunks)
```

```{python now feed the text chunks into a llm along with the question to answer}
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            You are a scientific research assistant.
            
            ## Task: 
            Answer a question about a scientific article given text extracts from the article. DO NOT MAKE ANYTHING UP. 
            
            """
        ),
        ("human", "Article text: {context}\n Question: {question}")
    ]
)


chain = prompt | model1 

result = chain.invoke({
    "context": related_chunks,
    "question": qry
})

print(result)

```




# Example 3: Code using a pre-trained classification model

Explore models available on HuggingFace 

```{python using a pretrained model for text classification - sentiment analysis}
sentiment_text = "Conserving blue carbon ecosystems can help prevent emissions that would have been released into the atmosphere from their degredation."

# Topic
topic_path = f"cardiffnlp/tweet-topic-latest-multi"
topic_task = pipeline(task = "text-classification", model = topic_path, tokenizer=topic_path, return_all_scores=True)

# Sentiment
sentiment_path='cardiffnlp/twitter-roberta-base-sentiment-latest'
sentiment_task = pipeline("sentiment-analysis", model=sentiment_path, tokenizer=sentiment_path)

topic_result = topic_task(sentiment_text)
flat_results = topic_result[0]  # assume single inner list
label_threshold = 0.5
labels = {item["label"] for item in flat_results if item["score"] > label_threshold}

print(labels)

sentiment_result = sentiment_task(sentiment_text)
sentiment_result = sentiment_result[0]  
print(sentiment_result)

```







